{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Sequential MNIST\n",
    "### RNN\n",
    "`cell = tf.nn.rnn_cell.LSTMCell(num_units=100, use_peepholes=True)`  \n",
    "で個別の層の設定終わり。  \n",
    "`output, state = tf.nn.dynamic_rnn(cell=cell, inputs=x, dtype=tf.float32)`\n",
    "で層全体の設定終わりで出力。  \n",
    "多層のRNNは1つ目のセルを何個か作って  \n",
    "`cell = tf.nn.rnn_cell.MultiRnnCell(cells=cells)`  \n",
    "のようにがっちゃんこする関数に渡すだけ。簡単すぎる。  \n",
    "ちなみに`tf.nn.dynamic_rnn`の他に`tf.nn.static_rnn`もあるけど、前者のほうが実装が楽だしモデル構築も高速なので本の中では前者を採用してるらしい。詳しい違いはリファレンスなり本文を確認されたし。  \n",
    "追記：[Qiitaに記事あった](https://qiita.com/suckgeun/items/c9c400f37979b3d4ca89)\n",
    "\n",
    "### データ\n",
    "MISTの画像はグレースケール（チャンネル数が１）なので、[高さ、幅、チャンネル]の3階テンソルを[高さ、幅]の２階テンソルとして扱うことができる。  \n",
    "また、画像サイズが固定なので、１行の画素データ群の時系列の入力として見ることができる。  \n",
    "（１行目を`t=1`、２行目を`t=2`とみなせる）  \n",
    "  \n",
    "### 注意点\n",
    "MNISTはRNNで学習させてもあまりメリットがなく、速度も精度も劣るが、データがある状態で実装を行い、RNNの実装そのものに慣れるのに最適なデータセットなので今回はMNISTでRNNを実装している。\n",
    "  \n",
    "（あとで実装するWord2Vecを見ればわかる通り、形態素解析などのデータの前処理が大変なので、いきなりそれをするとRNNに集中できない）  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mnist_rnn():\n",
    "    # importing MNIST data\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets('mnist_data/', one_hot=True)\n",
    "\n",
    "    # input data\n",
    "    num_seq = 28\n",
    "    num_input = 28\n",
    "\n",
    "    # need to transform binary data into [batch_size, num_seq, num_input]\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    input = tf.reshape(x, [-1, num_seq, num_input])\n",
    "\n",
    "    # setting LSTM cells with 128 units\n",
    "    # piled up to three stacks\n",
    "    stacked_cells = []\n",
    "    for i in range(3):\n",
    "        stacked_cells.append(tf.nn.rnn_cell.LSTMCell(num_units=128))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_cells)\n",
    "\n",
    "    # Creating RNN\n",
    "    # about outputs:\n",
    "    # https://stackoverflow.com/questions/48238113/tensorflow-dynamic-rnn-state\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=cell, inputs=input, dtype=tf.float32)\n",
    "\n",
    "    last_output = outputs[:, -1, :]\n",
    "\n",
    "    w = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    out = tf.nn.softmax(tf.matmul(last_output, w) + b)\n",
    "\n",
    "    # after creating rnn, train and test as usual\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(out), axis=[1]))\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        test_images = mnist.test.images\n",
    "        test_labels = mnist.test.labels\n",
    "\n",
    "        for i in range(1000):\n",
    "            step = i+1\n",
    "            train_images, train_labels = mnist.train.next_batch(50)\n",
    "            sess.run(train_step, feed_dict={x:train_images ,y:train_labels})\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                acc_val = sess.run( accuracy, feed_dict={x:test_images, y:test_labels})\n",
    "                print('Step {}: accuracy = {}'.format(step, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-00c255106689>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/tk/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/tk/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tk/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tk/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/tk/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Step 100: accuracy = 0.33640000224113464\n",
      "Step 200: accuracy = 0.5544999837875366\n",
      "Step 300: accuracy = 0.6705999970436096\n",
      "Step 400: accuracy = 0.6929000020027161\n",
      "Step 500: accuracy = 0.7354000210762024\n",
      "Step 600: accuracy = 0.8342000246047974\n",
      "Step 700: accuracy = 0.864300012588501\n",
      "Step 800: accuracy = 0.8924000263214111\n",
      "Step 900: accuracy = 0.9039000272750854\n",
      "Step 1000: accuracy = 0.9061999917030334\n"
     ]
    }
   ],
   "source": [
    "mnist_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "### date_set.py\n",
    "\n",
    "以下のコマンドでMeCabとnatto-pyのインストールを行っておくこと。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# install MeCab\n",
    "$ sudo apt-get install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
    "\n",
    "# install natto-py\n",
    "$ pip install natto-py\n",
    "\n",
    "# how to import\n",
    "from natto-py import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from natto import MeCab\n",
    "\n",
    "\n",
    "# loading files\n",
    "class DataSet(object):\n",
    "\n",
    "    def __init__(self, data_dir, max_vocab):\n",
    "\n",
    "        # 全データセットのファイルパスを取得\n",
    "        file_pathes = []\n",
    "        for file_path in glob.glob(data_dir+'*'):\n",
    "            file_pathes.append(file_path)\n",
    "\n",
    "        # ファイルを読み込み\n",
    "        row_documents = [self._read_docment(file_path)\n",
    "                         for file_path in file_pathes]\n",
    "        # 必要な部分だけ抽出\n",
    "        documents = [self._preprocessing(document)\n",
    "                     for document in row_documents]\n",
    "        # 形態素解析\n",
    "        splited_documents = [self._morphological(document)\n",
    "                             for document in documents]\n",
    "\n",
    "        words = []\n",
    "        for word_list in splited_documents:\n",
    "            words.extend(word_list)\n",
    "\n",
    "        # データセット作成\n",
    "        self.id_sequence, self.word_frequency, self.w_to_id, self.id_to_w \\\n",
    "            = self._build_data_sets(words, max_vocab)\n",
    "        print('Most common words (+UNK)', self.word_frequency[:5])\n",
    "        print('Sample data.')\n",
    "        print(self.id_sequence[:10])\n",
    "        print([self.id_to_w[i] for i in self.id_sequence[:10]])\n",
    "        self.data_index = 0\n",
    "\n",
    "    # ファイルの読み込み\n",
    "    def _read_docment(self, file_path):\n",
    "        with open(file_path, 'r', encoding='sjis') as f:\n",
    "            return f.read()\n",
    "\n",
    "    # ヘッダなどの不要データを前処理。必要な部分だけを返す。\n",
    "    def _preprocessing(self, document):\n",
    "\n",
    "        lines = document.splitlines()\n",
    "        processed_line = []\n",
    "\n",
    "        horizontal_count = 0\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            # ヘッダーは読み飛ばす\n",
    "            if horizontal_count < 2:\n",
    "                if line.startswith('-------'):\n",
    "                    horizontal_count += 1\n",
    "                continue\n",
    "            # フッターに入る行になったらそれ以降は無視\n",
    "            if line.startswith('底本：'):\n",
    "                break\n",
    "\n",
    "            line = re.sub(r'《.*》', '', line)  # ルビを除去\n",
    "            line = re.sub(r'［.*］', '', line)   #脚注を除去\n",
    "            line = re.sub(r'[!-~]', '', line)  # 半角記号を除去\n",
    "            line = re.sub(r'[︰-＠]', '', line)  # 全角記号を除去\n",
    "            line = re.sub('｜', '', line)  # 脚注の始まりを除去\n",
    "\n",
    "            processed_line.append(line)\n",
    "\n",
    "        return ''.join(processed_line)\n",
    "\n",
    "    # 形態素解析\n",
    "    def _morphological(self, document):\n",
    "\n",
    "        word_list = []\n",
    "        # MeCabの形態素解析結果のフォーマット\n",
    "        with MeCab('-F%f[0],%f[1],%f[6]') as mcb:\n",
    "            for token in mcb.parse(document, as_nodes=True):\n",
    "                features = token.feature.split(',')\n",
    "                # 名詞（一般）動詞（自立）、形容詞（自立）以外は除外\n",
    "                if features[0] == '名詞' \\\n",
    "                        and features[1] == '一般' \\\n",
    "                        and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                if features[0] == '動詞' \\\n",
    "                        and features[1] == '自立' \\\n",
    "                        and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                if features[0] == '形容詞' \\\n",
    "                        and features[1] == '自立' \\\n",
    "                        and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "        return word_list\n",
    "\n",
    "    # 辞書作成\n",
    "    def _build_data_sets(self, words, max_vocab):\n",
    "\n",
    "        # 単語出現回数を解析。\n",
    "        # 出現数が少ない単語をUnknown wordとしてひとくくりに扱う\n",
    "        word_frequency = [['UNW', -1]]\n",
    "        word_frequency.extend(collections.Counter(words)\n",
    "                              .most_common(max_vocab - 1))\n",
    "        # 単語=>IDの辞書\n",
    "        w_to_id = dict()\n",
    "        for word, _ in word_frequency:\n",
    "            w_to_id[word] = len(w_to_id)\n",
    "        # 形態素解析した文章を単語IDの並びに変換\n",
    "        id_sequence = list()\n",
    "        unw_count = 0\n",
    "        for word in words:\n",
    "            # UNK処理\n",
    "            if word in w_to_id:\n",
    "                index = w_to_id[word]\n",
    "            else:\n",
    "                index = 0\n",
    "                unw_count += 1\n",
    "            id_sequence.append(index)\n",
    "        word_frequency[0][1] = unw_count\n",
    "        # 単語ID=>単語の辞書\n",
    "        id_to_w = dict(zip(w_to_id.values(), w_to_id.keys()))\n",
    "        return id_sequence, word_frequency, w_to_id, id_to_w\n",
    "\n",
    "    # num_skip:１つの入力をどれだけ再利用するか\n",
    "    # skip_window: 左右何語までを正解対象にするか\n",
    "    def create_next_batch(self, batch_size, num_skips, skip_window):\n",
    "\n",
    "        assert batch_size % num_skips == 0\n",
    "        # 一つの入力の再利用回数が対象範囲全件を超えてはならない\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        inputs = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        span = 2 * skip_window + 1\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        # データセットが1週しそうならindexを最初にもどす\n",
    "        if self.data_index + span > len(self.id_sequence):\n",
    "            self.data_index = 0\n",
    "        # 初期のqueueを構築(window内の単語をすべて格納)\n",
    "        buffer.extend(self.id_sequence[self.data_index:self.data_index+span])\n",
    "        self.data_index += span\n",
    "\n",
    "        for i in range(batch_size // num_skips):\n",
    "            # 中心は先に正解データから除外\n",
    "            target = skip_window\n",
    "            targets_to_avoid = [skip_window]\n",
    "            for j in range(num_skips):\n",
    "                # すでに選ばれている物以外から正解データのインデックスを取得\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                # 次回以降targetにならないように\n",
    "                targets_to_avoid.append(target)\n",
    "                # 入力値になるのはbufferの中心\n",
    "                inputs[i * num_skips + j] = buffer[skip_window]\n",
    "                # ランダムに指定した周辺単語が正解データに\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "\n",
    "            # 次に入れる単語がデータセットにない場合はbufferには最初の値を入力\n",
    "            if self.data_index == len(self.id_sequence):\n",
    "                buffer = self.id_sequence[:span]\n",
    "                self.data_index = span\n",
    "            else:\n",
    "                # bufferに次の単語を追加してindexを1進める\n",
    "                buffer.append(self.id_sequence[self.data_index])\n",
    "                self.data_index += 1\n",
    "        # 最後の方のデータが使われないことを避けるために少しだけindexを元に戻す\n",
    "        self.data_index = (self.data_index + len(self.id_sequence) - span) \\\n",
    "            % len(self.id_sequence)\n",
    "\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec.py\n",
    "#### FLAGS\n",
    "`tf.app.flags.FLAGS`を利用すれば、`argparse`のようにプログラムを実行する際に引数を用いることができるようになる。  \n",
    "  \n",
    "なぜ必要か？：  \n",
    "ディープラーニングではハイパーパラメーターをいくつも設定する必要があり、それを変更する際にプログラムをいちいち変更するのは面倒なので、ある程度ハイパーパラメーターが多くなった場合にはFLAGSを定義して引数を受け取れるようにするのが便利。  \n",
    "  \n",
    "#### 参考リンク  \n",
    "- [What's the purpose of tf.app.flags in TensorFlow?\n",
    "](https://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow)\n",
    "- [TensorFlow – tf.app.flags.FLAGS（ファイル実行時にパラメタを付与できるようにする）\n",
    "](https://endoyuta.com/2017/01/20/tensorflow-tf-app-flags-flags%EF%BC%88%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E5%AE%9F%E8%A1%8C%E6%99%82%E3%81%AB%E3%83%91%E3%83%A9%E3%83%A1%E3%82%BF%E3%82%92%E4%BB%98%E4%B8%8E%E3%81%A7%E3%81%8D/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'data_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-20be5890f66d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data set directory.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'data_set'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from data_set import *\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('data_dir', 'data/', \"Data set directory.\")\n",
    "tf.app.flags.DEFINE_string('log_dir', 'logs/', \"Log directory.\")\n",
    "tf.app.flags.DEFINE_integer('max_vocab', 2000, \"Max Vocablary size.\")\n",
    "tf.app.flags.DEFINE_integer('skip_window', 2,\n",
    "                            \"How many words to consider left and right.\")\n",
    "tf.app.flags.DEFINE_integer('num_skips', 4,\n",
    "                            \"How many times to reuse input to get labels.\")\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 64,\n",
    "                            \"Dimension of the embedding vector.\")\n",
    "tf.app.flags.DEFINE_integer('num_sumpled', 64,\n",
    "                            \"Number of negative examples to sample.\")\n",
    "tf.app.flags.DEFINE_integer('num_step', 10000, \"Train step.\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Batch size.\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.1, \"Learning rate.\" )\n",
    "tf.app.flags.DEFINE_bool('create_tsv', True, \"Create words.tsv or not.\" )\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    # データセットオブジェクトを作成\n",
    "    data = DataSet(FLAGS.data_dir, FLAGS.max_vocab)\n",
    "\n",
    "    # Embeddingsように使うラベルをtsv形式で保存\n",
    "    if FLAGS.create_tsv:\n",
    "        sorted_dict = sorted(data.w_to_id.items(), key=lambda x: x[1])\n",
    "        words = [\"{word}\\n\".format(word=x[0]) for x in sorted_dict]\n",
    "        with open(FLAGS.log_dir+\"words.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "            f.writelines(words)\n",
    "        print(\"Embeddings metadata was saved to \"+FLAGS.log_dir+\"/words.tsv\")\n",
    "\n",
    "    batch_size = FLAGS.batch_size\n",
    "    embedding_size = FLAGS.embedding_size\n",
    "    vocab_size = len(data.w_to_id)\n",
    "    # placeholderの定義\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    correct = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    word_embedding = tf.Variable(tf.random_uniform(\n",
    "        [vocab_size, embedding_size], -1.0, 1.0), name='word_embedding')\n",
    "    embed = tf.nn.embedding_lookup(word_embedding, inputs)\n",
    "    w_out = tf.Variable(tf.truncated_normal(\n",
    "        [vocab_size, embedding_size], stddev=1.0/math.sqrt(embedding_size)))\n",
    "    b_out = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n",
    "    nce_loss = tf.nn.nce_loss(weights=w_out, biases=b_out, labels=correct,\n",
    "                              inputs=embed, num_sampled=FLAGS.num_sumpled,\n",
    "                              num_classes=vocab_size)\n",
    "    loss = tf.reduce_mean(nce_loss)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\\\n",
    "        .minimize(loss, global_step=global_step)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_dir)\n",
    "        if ckpt_state:\n",
    "            last_model = ckpt_state.model_checkpoint_path\n",
    "            saver.restore(sess,last_model)\n",
    "            print(\"model was loaded:\", last_model)\n",
    "        else:\n",
    "            sess.run(init)\n",
    "            print(\"initialized.\")\n",
    "\n",
    "        last_step = sess.run(global_step)\n",
    "        average_loss = 0\n",
    "        for i in range(FLAGS.num_step):\n",
    "\n",
    "            step = last_step + i + 1\n",
    "            batch_inputs, batch_labels = data.create_next_batch(\n",
    "                batch_size, FLAGS.num_skips, FLAGS.skip_window)\n",
    "            feed_dict = {inputs: batch_inputs, correct: batch_labels}\n",
    "\n",
    "            _, loss_val = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                average_loss /= 100\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "                saver.save(sess, FLAGS.log_dir+'my_model.ckpt', step)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
