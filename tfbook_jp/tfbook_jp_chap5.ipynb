{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Image Captioning\n",
    "→CV分野の中でも特に難しい分野。  \n",
    "物体検知のみならず、被写体同士の関係性を考慮して、それを文章に出力する必要があるから。  \n",
    "代表的なモデルはGoogleが2015年に発表した\"Show and Tell\"  \n",
    "画像分類のために訓練されたCNNを用いて入力画像を順伝播させ、出力層の直前の特徴を抽出してRNNへの入力とし、そのRNNがキャプションを作成するというもの。  \n",
    "このCNNはすでに学習済みの重みを利用しており、このように訓練されたモデルを別のタスクに利用することを転移学習と呼ぶ。  \n",
    "\n",
    "## 5.3 大規模データセットを扱う際の注意点\n",
    "まとめ  \n",
    "- 大量の画像データとメタデータを整形してバイナリに固める。\n",
    "- そのバイナリを逐次読み込んで別スレッドでミニバッチを作成してキューに格納するようにする。\n",
    "  \n",
    "今までは全てのデータセットを`placeholder`経由で入力・正解データのミニバッチの受け渡しをしてきたが、今回のように数十GBのデータセットでは全てのデータを一度にメモリにロードすることはできない。  \n",
    "そのため、今までのように`placeholder`経由でやり取りをする場合には、ミニバッチはディスクからその都度データを読み込んで加工して作成する必要がある。  \n",
    "しかし、そうするとこれまでは毎ステップメモリアクセスで済んでいたデータ読み込みにディスクIOが発生してしまう。  \n",
    "ディスクIOはメモリアクセスの100分の1から1,000文の1程度の速度なので、これが訓練の毎ステップに発生してしまうと、どれだけハイスペックなハードを使って効率的に訓練を行ったとしても、相当な学習時間を要してしまう。  \n",
    "    \n",
    "これを解消するために、ディスクからデータをロードしてデータセットを生成するスレッドと訓練を行うスレッドを分割して、キューイングをうまく行うことで、訓練を行っている間にミニバッチを作成するようにするのが定石。\n",
    "  \n",
    "また、もう一つの工夫として、今回の例のように一つ一つは容量が小さい画像データが数十万あるようなケースだと、画像データをひとつが100MB程度になるバイナリにまとめて、それから読み込むほうが効率的。  \n",
    "大量の小さい容量を逐次読み込むのは総容量が同じで1つのファイルを読み込むよりも遥かに時間がかかる。  \n",
    "  \n",
    "この骨が折れるような作業も、簡単かつ高速で処理できるAPIがTensorFlowには用意されている。\n",
    "\n",
    "## 5.4 TFRecord形式によるデータのバイナリ化\n",
    "本来バイナリ化は、仕様を細かく決めた上で慎重にバイト数を意識してデータのエンコード・デコードを行うことが必要だが、`TFRecord`を使えば簡単にデータの読み書きが行える。  \n",
    "他の方法でもバイナリ化は可能だが、**TensorFlowは`TFRecord`を推奨しており、他の方法と比べて高速でデータの読み書きが可能である。**  \n",
    "  \n",
    "### Writing\n",
    "`TFRecord`形式でファイルを書き込む基本的な方法として、1件のデータセットに相当する`tf.train.Example()`クラスに必要なデータを登録して、それを専用の`writer`を用いて書き込むという手順を取る。  \n",
    "  \n",
    "`tf.train.Features()`（複数形）はキーバリュー形式での登録を行うクラスで、`tf.train.Feature()`（単数形）は1つのバリューを定義するクラスなので注意する。  \n",
    "データの型によって後者の引数が変わるので、詳しくはリファレンスか本書を参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Get a file path\n",
    "IMG_LIST = [i for i in glob.glob('img/*.jpg')]\n",
    "\n",
    "# Export as 'test.tfrecord'\n",
    "with tf.python_io.TFRecordWriter('test.tfrecord') as w:\n",
    "    for img in IMG_LIST:\n",
    "\n",
    "        # 'rb' indicates to read files as binary data\n",
    "        with tf.gfile.FastGFile(img, 'rb') as f:\n",
    "            data = f.read()\n",
    "\n",
    "        # Resister read bytes to key and value\n",
    "        features = tf.train.Features(feature={\n",
    "            'data': tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "                value=[data]))\n",
    "        })\n",
    "\n",
    "        # Write into Example Class\n",
    "        example = tf.train.Example(features=features)\n",
    "        w.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading\n",
    "`TFRecord`ファイルを読み込む方法はいくつかあり、`TFRecord`専用の`reader`も存在するが、ここではTensorFlowのDataSetAPIを使って解説を行う。  \n",
    "DataSetAPIとはモデルの読み込み、加工、バッチ化、シャッフルなどの機械学習におけるデータセット周りで必須の面倒な処理を簡単に記述できる非常に強力なAPIのこと。  \n",
    "  \n",
    "DataSetAPIには2つの重要なクラスが存在する。  \n",
    "`tf.data.Dataset()`クラスと、`tf.data.Iterator()`クラス。\n",
    "  \n",
    "- Dataset　・・・　データセットの集合を管理し、加工やバッチ化を行う。\n",
    "- Iterator　・・・  Datasetオブジェクトをどのようにループさせるかを管理する。\n",
    "  \n",
    "`Dataset`クラスの`from_tesnors()`や`from_tensor_slices()`メソッドを用いればTesnorやTensorの配列からデータセットを読み込むことができる。  \n",
    "今回のように`TFRecord`形式のデータを読み込む際は`tf.data.TFRecordDataset()`クラスを用いる。  \n",
    "引数は`TFRecord`のファイル名一覧で、普通は全データ容量が大きく、１００Mb程度の大きさに分割してバイナルファイルを管理するので、その際には引数がファイル名の配列になる。  \n",
    "`map()`メソッドを用いるとデータセット1レコードごとにどのような処理を食われるかということを記述できる。  \n",
    "  \n",
    "パース処理では`ｔｆ．FixedLegFeature()`を用いる。  \n",
    "第一引数はパース対象のTensorのshape。  \n",
    "パースしただけではまだバイト列のままなので、`tf.image.decode_jpeg()`オペレーションを用いることで、unit8型の3階テンソルにしている。  \n",
    "  \n",
    "データセットの用意が完了したので、あとは`Iterator` クラスを用いてどのようにループさせるかを定義する。  \n",
    "`make_one_shot_iterator()`メソッドは順番にデータセットを一周するという基本的な`Iterator`。  \n",
    "そして、`get_next()`メソッドを用いて1件ずつデータセットにアクセスするためのTensorを取得する。\n",
    "  \n",
    "**下のコードは単体ではエラーが出て動かないので注意（実装方法の確認用）**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67ae2c008835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Define iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_hot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Get an element from iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "\n",
    "def parse(example):\n",
    "    # Parse TFRecord\n",
    "    features = tf.parse_single_example(\n",
    "        example,\n",
    "        features={\n",
    "            'data': tf.FixedLenFeature([], dtype=tf.string)\n",
    "        })\n",
    "\n",
    "    # Decode bytes into jpeg\n",
    "    img = features['data']\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    return img\n",
    "\n",
    "# Read and parse TFRecord data\n",
    "data_set = tf.data.TFRecordDataset(['test.tfrecord']).map(parse)\n",
    "\n",
    "# Define iterator\n",
    "iterator = dataset.make_one_hot_iterator()\n",
    "\n",
    "# Get an element from iterator\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Get sigle image from dataset\n",
    "    jpeg_img = sess.run(next_element)\n",
    "\n",
    "    # Show jpeg_img with sckit-img\n",
    "    io.imshow(jpeg_img)\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 データセット整形プログラム作成\n",
    "  \n",
    "流れは以下の通り。  \n",
    "\n",
    "1. STAIR Captionsのファイルを読み込んで画像idからファイルパスとキャプション用の辞書を作成する\n",
    "2. 訓練用、検証用、テスト用にデータを分割する\n",
    "3. 訓練用のデータを元に単語辞書を作成する\n",
    "4. 画像を読み込み`TFRecord`形式で画像とキャプションの組み合わせを1レコードとして`TFrecord`に書き込む\n",
    "  \n",
    "加えて、1つの`TFRecord`に全てのデータを書き込んでしまうとデータが大きくなりすぎてしまうために、データを分割して複数の`TFRecord`ファイルに書き込む。  \n",
    "  \n",
    "今回使用するのは、JSONファイルの中の`image`と`annotation`のみ。\n",
    "`image`が各画像ファイルのファイル名や画像サイズとった情報を持っており、`annotation`がキャプション情報を保持している。  \n",
    "  \n",
    "### dataset.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "# dataset.py from\n",
    "# https://github.com/thinkitcojp/TensorFlowDL-samples\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import namedtuple, Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.flags.DEFINE_string(\"train_img_dir\", \"data/img/train2014/\", \"Training image directory.\")\n",
    "tf.flags.DEFINE_string(\"val_img_dir\", \"data/img/val2014/\", \"Validation image directory.\")\n",
    "tf.flags.DEFINE_string(\"train_captions\", \"data/stair_captions_v1.1_train.json\", \"Training caption file.\")\n",
    "tf.flags.DEFINE_string(\"val_captions\", \"data/stair_captions_v1.1_val.json\", \"Validation caption file.\")\n",
    "tf.flags.DEFINE_string(\"out_dir\", \"data/tfrecords/\", \"Output TFRecords directiory.\")\n",
    "tf.flags.DEFINE_integer(\"min_word_count\", 4, \"The minimum number of occurrences of each word in th training set for includion in the vocab.\")\n",
    "tf.flags.DEFINE_string(\"word_list_file\", \"data/dictionary.txt\", \"Output word list file.\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "START_WORD = '<S>'\n",
    "END_WORD = '<E>'\n",
    "UNKNOWN_WORD = '<UNW>'\n",
    "\n",
    "NUM_TRAIN_FILE = 256\n",
    "NUM_VAL_FILE = 4\n",
    "NUM_TEST_FILE = 8\n",
    "\n",
    "\n",
    "ImageMetadata = namedtuple(\"ImageMetadata\",[\"img_id\", \"filename\"])\n",
    "\n",
    "#画像メタデータと辞書をもとに、指定されたファイル数に分割してバイナリ（TFRecord）を作成する\n",
    "def _create_datasets(name, img_meta, captions, word_to_id, num_file):\n",
    "\n",
    "    #画像メタデータをだいたい等しく分割\n",
    "    img_chunk = np.array_split(img_meta, num_file)\n",
    "    counter = 0\n",
    "    for i in range(1, num_file + 1):\n",
    "        output_file_name = \"%s-%.3d.tfrecord\" % (name, i)\n",
    "        output_file_path = os.path.join(FLAGS.out_dir, output_file_name)\n",
    "        target_chunk = img_chunk[counter]\n",
    "        #対象画像群書ごとにWriterを定義\n",
    "        with tf.python_io.TFRecordWriter(output_file_path) as writer:\n",
    "            for img in target_chunk:\n",
    "                img_id = img[0]\n",
    "                filename = img[1]\n",
    "                #画像ファイルをバイト列として読み込み\n",
    "                with tf.gfile.FastGFile(filename, \"rb\") as f:\n",
    "                    data = f.read()\n",
    "\n",
    "                #キャプションのid化\n",
    "                caption = captions[int(img_id)]\n",
    "                caption_ids = []\n",
    "                for w in caption:\n",
    "                    if w in word_to_id:\n",
    "                        caption_ids.append(word_to_id[w])\n",
    "                    else:\n",
    "                        caption_ids.append(word_to_id[UNKNOWN_WORD])\n",
    "\n",
    "                #固定長部分\n",
    "                context = tf.train.Features(feature={\n",
    "                    \"img_id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[int(img_id)])),\n",
    "                    \"data\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data])),\n",
    "                    })\n",
    "\n",
    "                #可変長部分\n",
    "                caption_feature = [tf.train.Feature(int64_list=tf.train.Int64List(value=[v])) for v in caption_ids]\n",
    "                feature_lists = tf.train.FeatureLists(feature_list={\n",
    "                    \"caption\":tf.train.FeatureList(feature=caption_feature)\n",
    "                    })\n",
    "\n",
    "                #TFRecordに書き込み\n",
    "                sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists)\n",
    "                writer.write(sequence_example.SerializeToString())\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "#jsonファイルを読み込み画像のid, ファイル名, キャプションを取得する。\n",
    "def _load_metadata(caption_filename, img_dir):\n",
    "\n",
    "    #jsonファイルをロード\n",
    "    with open(caption_filename, 'r') as f:\n",
    "        meta_data = json.load(f)\n",
    "\n",
    "    #画像idとファイル名を持つnamedtupleのリストを作成\n",
    "    meta_list = [ImageMetadata(x['id'], os.path.join(img_dir, x['file_name'])) for x in meta_data['images']]\n",
    "\n",
    "    #スペース区切りのcaptionを単語の配列に変換\n",
    "    def _create_word_list(caption):\n",
    "        tokenized_captions = [START_WORD]\n",
    "        tokenized_captions.extend(caption.split())\n",
    "        tokenized_captions.append(END_WORD)\n",
    "        return tokenized_captions\n",
    "\n",
    "    #{画像id => キャプションのリスト}の辞書を作成\n",
    "    id_to_captions = {}\n",
    "    for annotation in meta_data[\"annotations\"]:\n",
    "        img_id = annotation['image_id']\n",
    "        caption = annotation['tokenized_caption']\n",
    "        caption = _create_word_list(caption)\n",
    "        #キャプションはいくつかあるため１つだけを採用\n",
    "        id_to_captions[img_id] = caption\n",
    "\n",
    "    print(\"Loaded caption metadata for %d images from %s\" % (len(meta_list), caption_filename))\n",
    "\n",
    "    return meta_list, id_to_captions\n",
    "\n",
    "\n",
    "def _create_vocab(captions):\n",
    "\n",
    "    counter = Counter()\n",
    "    for c in captions:\n",
    "        counter.update(c)\n",
    "\n",
    "    print(\"total words:\", len(counter))\n",
    "    #出現回数が一定数のものだけ辞書に採用。出現回数降順でソート\n",
    "    #word_countsは(単語, 出現回数)のリスト\n",
    "    word_counts = [x for x in counter.items() if x[1] >= FLAGS.min_word_count]\n",
    "    word_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"Words in vocab:\", len(word_counts))\n",
    "\n",
    "\n",
    "    #辞書作成\n",
    "    word_list = [x[0] for x in word_counts]\n",
    "    #<S>と<E>のidを1,0で固定したいので、一度削除して先頭に追加する\n",
    "    word_list.remove(START_WORD)\n",
    "    word_list.remove(END_WORD)\n",
    "    word_list.insert(0, START_WORD)\n",
    "    word_list.insert(0, END_WORD)\n",
    "\n",
    "    word_list.append(UNKNOWN_WORD)\n",
    "    word_to_id = dict([(x, y) for (y, x) in enumerate(word_list)])\n",
    "    id_to_word = dict([(x, y) for (x, y) in enumerate(word_list)])\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    #jsonファイルからメタデータの読み込み\n",
    "    #(画像id, ファイルパス)のタプルの配列と{id=>キャプションのリスト}を取得\n",
    "    train_meta, train_captions = _load_metadata(FLAGS.train_captions, FLAGS.train_img_dir)\n",
    "    val_meta, val_captions = _load_metadata(FLAGS.val_captions, FLAGS.val_img_dir)\n",
    "\n",
    "    #キャプションをマージ\n",
    "    captions = {k:v for dic in [train_captions, val_captions] for k, v in dic.items()}\n",
    "\n",
    "    #訓練データ,バリデーションデータ,テストデータに分割\n",
    "    train_cutoff = int(0.85 * len(val_meta))\n",
    "    val_cutoff = int(0.90 * len(val_meta))\n",
    "\n",
    "    train_dataset = train_meta + val_meta[0:train_cutoff]\n",
    "    val_dataset = val_meta[train_cutoff:val_cutoff]\n",
    "    test_dataset = val_meta[val_cutoff:]\n",
    "\n",
    "\n",
    "    #訓練データから辞書作成\n",
    "    train_captions = []\n",
    "    for meta in train_dataset:\n",
    "        c = captions[meta.img_id]\n",
    "        train_captions.append(c)\n",
    "\n",
    "    word_to_id, id_to_word = _create_vocab(train_captions)\n",
    "\n",
    "\n",
    "    #画像を読み込みメタデータと結合したバイナリを作成\n",
    "    _create_datasets(\"train\", train_dataset, captions, word_to_id, NUM_TRAIN_FILE)\n",
    "    _create_datasets(\"val\", val_dataset, captions, word_to_id, NUM_VAL_FILE)\n",
    "    _create_datasets(\"test\", test_dataset, captions, word_to_id, NUM_TEST_FILE)\n",
    "\n",
    "    # 単語リスト出力\n",
    "    with open(FLAGS.word_list_file, 'a') as f:\n",
    "        for k, v in id_to_word.items():\n",
    "            f.write(v)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### namedtuple\n",
    "この[サイト](http://d.hatena.ne.jp/pknight/20170323/1490234933)がわかりやすい\n",
    "使いこなせたらすごく便利そう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "TEST = namedtuple('TEST', ['X', 'Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TEST(X=0, Y=0), TEST(X=1, Y=1), TEST(X=2, Y=2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [TEST(i, i) for i in range(3)]\n",
    "\n",
    "test  # [TEST(X=0, Y=0), TEST(X=1, Y=1), TEST(X=2, Y=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST(X=0, Y=0)\n",
      "TEST(X=1, Y=1)\n"
     ]
    }
   ],
   "source": [
    "print(test[0])  # TEST(X=0, Y=0)\n",
    "print(test[1])  # TEST(X=1, Y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Transfer Learning with GoogLeNet Inception-v3\n",
    "### Inception-v3\n",
    "Deep Learningを利用するとどうしても、「畳み込みサイズのフィルタのサイズやユニット数はどうしたら良いのか？」など、構造をどうした方がより良いモデルになるのかという疑問がつきまとう。  \n",
    "しかもその結論は「どの値にしてもそれぞれ別の重要な特徴量を抽出することができる」である。  \n",
    "そこで、「いくつかの異なるパターンを試してそれを結合してしまおう」というのがInceptionモジュールの発想らしい。  \n",
    "応用例：[ラーメン二郎を識別する人工知能の中身](https://qiita.com/shouta-dev/items/26cb1ace8c11c196f86e)  \n",
    "  \n",
    "Inceptionについての詳しい説明は省略。  \n",
    "  \n",
    "### Inception-v3の書き出し \n",
    "Inception-v3を利用する方法はいくつかあるが、一番手っ取り早いのは配布済みのProtocol Buffers形式（pb形式）のファイルを用いること。  \n",
    "他には、TensorFlowのハイレベルAPIである`tf.contrib.slim`パッケージを用いて書かれたInception-v3のコードを読み込んで、公開済みのチェックポイントファイルをロードして用いる方法もある。  \n",
    "前者の場合は現在公開されているモデルが入力を一見しか受け付けない推論に特化した形になっているため、今回は後者を採用。  \n",
    "ただ、今回の学習では画像側のネットワークは固定にしてしまいたいため、モデルとチェックポイントファイルから`Variable`を`Constant`に変換してpd形式に固め直す。  \n",
    "   \n",
    "### create_inception_v3_pb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# create_inception_v3_pb.py from\n",
    "# https://github.com/thinkitcojp/TensorFlowDL-samples\n",
    "# Requiring inception_v3.py located in the same dir to excecute this file.\n",
    "\n",
    "import tensorflow as tf\n",
    "import inception_v3 as iv3\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "tf.flags.DEFINE_string(\"ckpt_file\", 'ckpt/inception_v3.ckpt', \"Inception-v3 checkpoint file.\")\n",
    "tf.flags.DEFINE_string('log_dir', 'logs/', \"TensorBoard log directory.\")\n",
    "tf.flags.DEFINE_string('output_dir', './', \"Output directory.\")\n",
    "tf.flags.DEFINE_string('output_file', 'inception_v3.pb', \"Output file name.\")\n",
    "\n",
    "#　Inception-v3を読み込み\n",
    "input_img = tf.placeholder(tf.float32,[None, 299, 299, 3], name='input_image')\n",
    "arg_scope = iv3.inception_v3_arg_scope()\n",
    "with tf.contrib.slim.arg_scope(arg_scope):\n",
    "    logits, end_points = iv3.inception_v3(inputs=input_img, is_training=False, num_classes=1001)\n",
    "\n",
    "#　計算グラフ取得\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#　TensorBordで確認できるように\n",
    "writer = tf.summary.FileWriter(FLAGS.log_dir, graph)\n",
    "writer.close()\n",
    "\n",
    "#　pb形式で書き出し\n",
    "tf.train.write_graph(graph, FLAGS.output_dir, FLAGS.output_file)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #チェックポイントが読み込めるか念のために確認\n",
    "    saver.restore(sess, FLAGS.ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`arg_scope`は`tf.contrib.slim`パッケージで出てくる引数の統一スコープ。  \n",
    "毎回引数に`tf.nn.relu`などを記載するのは面倒難度絵、`arg_scope`で一回宣言すれば、同じ名前の引数には同じ引数を与えてくれるという機能を提供している。  \n",
    "ここでは、inception_v3.pyに記載されているデフォルトの`arg_scope`をロードしている。  \n",
    "今回は再度訓練するわけではないので、引数`is_training`は`False`にする。  \n",
    "`num_classes=1001`にしている理由は、チェックポインファイルのモデルが出力層のユニット数を1001にしているから。  \n",
    "デフォルトの1000にしてしまうと、チェックポイントのファイルが読み込めないらしい。  \n",
    "  \n",
    "ここで`FileWriter`をすぐにｃｌｏｓｅしているのは、訓練のログを取るわけではなく、計算グラフが正しく載っているか確認するためだから。  \n",
    "  \n",
    "グラフファイルの書き出しは`tf.train.write_graph()`メソッドで行うことができる。  \n",
    "最後に念の為、チェックポイントファイルがこのグラフに合致しているものなのかを`tf.train.Saver()`クラスの`restore()`メソッドでロードして確認している。  \n",
    "もし全く違うファイル、バージョンなどが異なるファイルをロードした場合はエラーになるため、デバックの代わりになる。\n",
    "\n",
    "### freeze_graph.py\n",
    "freeze_graph.pyは計算グラフが記載されたpbファイルとチェックポイントファイルを読み込んで、計算グラフ上の`ｔｆ．Variable()`をチェックポイントに記載された値に書き換えた`tf.constant()`に変換して永久保存版のモデルを作成するためのツール。  \n",
    "これによって出力されたpbファイルはpythonが乗っていないモバイル状やその他のデバイス上で実行できる。  \n",
    "実際に訓練したモデルを実環境に動かすときには、推論にしか用いない最小限の計算グラフを構築した後に、チェックポイントとファイルと一緒にfreeze_graph.pyで固めたpbファイルを用いる。  \n",
    "  \n",
    "freeze_graph.pyは以下の4つの引数を取る。  \n",
    "  \n",
    "- `input_graph`: 入力となる計算グラフのpbファイル名\n",
    "- `input_checkpoint`: 学習済みのチェックポイントファイル\n",
    "- `output_node_names`: 実行用pb形式として出力したいTensorの名前\n",
    "- `output_graph`: 出力pbファイル名\n",
    "  \n",
    "今回は`output_node_names`には、Softmax層の直前の値を指定する。\n",
    "ただし、今回のようにモデルのコードを自分で書いたわけではない場合、`output_node_names`に指定する値を長々としたコードから探すのは手間。  \n",
    "そういうときに、TensorBoardのGRAPHSを利用するとすぐに確認できる。  \n",
    "\n",
    "### im2txt.py\n",
    "詳細は本文を確認されたし。  \n",
    "\n",
    "#### Teacher Forcing\n",
    "学習初期はあてずっぽうな単語を生成するため、長期時系列の学習は非常に時間がかかる。  \n",
    "そのため、正解がわかっている学習のときには各時刻のRNNの入力として前の時刻の正解データを入れてあげることで学習速度を加速させる。  \n",
    "これをTeacher Forcingという。  \n",
    "https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/  \n",
    "https://satopirka.com/2018/02/encoder-decoder%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8teacher-forcingscheduled-samplingprofessor-forcing/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# im2txt.py from\n",
    "# https://github.com/thinkitcojp/TensorFlowDL-samples\n",
    "# This file requires PIL to run\n",
    "# Install by running '$ pip install Pillow' if you don't have PIL\n",
    "\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "tf.flags.DEFINE_string('tfrecord_dir', 'data/tfrecords/', \"TFRecords' directory.\")\n",
    "tf.flags.DEFINE_string('dictionary_path', 'data/dictionary.txt', \"Dictionary file path.\")\n",
    "tf.flags.DEFINE_string('inference_pb', 'data/im2txt.pb', \"Inference Graph pb file path.\")\n",
    "tf.flags.DEFINE_string('img_embedding_pb', 'create_inception_v3/inception_v3_freezed.pb',\"Image embedding network pb file path.\")\n",
    "tf.flags.DEFINE_string('model_dir', 'ckpt/', \"Saved checkpoint directory.\")\n",
    "tf.flags.DEFINE_string('log_dir', 'logs/', \"TensorBoard log directory.\")\n",
    "tf.flags.DEFINE_string('test_img_dir','data/img/for_eval/' ,\"Test image directory.\")\n",
    "tf.flags.DEFINE_integer('max_step', 100000, \"Num of max train step.\")\n",
    "tf.flags.DEFINE_integer('eval_interval', 100, \"Step interval of evaluation.\")\n",
    "tf.flags.DEFINE_integer('embedding_size', 512, \"Num of embedded feature size.\")\n",
    "tf.flags.DEFINE_integer('batch_size', 64, \"Num of batch size.\")\n",
    "tf.flags.DEFINE_float('learning_rate', 0.0005, \"Learning rate.\")\n",
    "tf.flags.DEFINE_float('max_gradient_norm', 5.0, \"Max norm of gradient.\")\n",
    "\n",
    "EOS_ID = 0\n",
    "SOS_ID = 1\n",
    "\n",
    "def _process_img(encoded_img):\n",
    "\n",
    "    #画像のデコード [height, width, channel]の3階テンソルになる\n",
    "    img = tf.image.decode_jpeg(encoded_img)\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "    #サイズが299*299になるようにリサイズ、クロップ\n",
    "    img = tf.image.resize_images(img, [373, 373])\n",
    "    img = tf.image.central_crop(img, 0.8)\n",
    "    return img\n",
    "\n",
    "def _parse_function(sequence_expample_proto):\n",
    "\n",
    "    #tf.train.SequenceExampleをパースするメソッド\n",
    "    context, feature_lists = tf.parse_single_sequence_example(\n",
    "        sequence_expample_proto,\n",
    "        context_features = {\n",
    "            \"img_id\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "            \"data\": tf.FixedLenFeature([], dtype=tf.string),\n",
    "        },\n",
    "        sequence_features = {\n",
    "            \"caption\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        })\n",
    "    #パース結果を格納、画像は加工して格納\n",
    "    img = _process_img(context['data'])\n",
    "    caption = feature_lists['caption']\n",
    "\n",
    "    #正解データのサイズを取得、正解データは<SOS>がないので1を引く\n",
    "    lengths = tf.size(caption) - 1\n",
    "    #訓練時decoderへの入力は<EOS>がない\n",
    "    decoder_input = caption[:-1]\n",
    "    #正解データは<SOS>がない\n",
    "    correct = caption[1:]\n",
    "    \n",
    "    return (img, lengths, decoder_input, correct)\n",
    "\n",
    "\n",
    "\n",
    "def build_input(pattern, repeat_count=None ,shuffle=True):\n",
    "\n",
    "    #訓練用、バリデーション用のtfrecordsファイル名一覧を取得\n",
    "    files = []\n",
    "    for file_path in glob.glob(FLAGS.tfrecord_dir+pattern):\n",
    "        files.append(file_path)\n",
    "\n",
    "    #TFRecordをパースしてDatasetを作成\n",
    "    dataset = tf.data.TFRecordDataset(files).map(_parse_function).repeat(count=repeat_count)\n",
    "\n",
    "    #1000件ずつバッファを取りながらデータをシャッフル\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    #キャプションデータ長の不均衡を0でpaddingしてミニバッチを作成\n",
    "    padded_shapes = (tf.TensorShape([300,300,3]), tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([None]))\n",
    "    dataset = dataset.padded_batch(FLAGS.batch_size, padded_shapes=padded_shapes)\n",
    "\n",
    "    #repeatで指定した回数までループするiteratorを作成\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "\n",
    "    return next_element\n",
    "\n",
    "def build_img_embedding(img_input, embedding_size):\n",
    "\n",
    "    #画像が崩れていないかTensorBoardで確認\n",
    "    tf.summary.image('input', img_input*256, max_outputs=10)\n",
    "\n",
    "    #inception-v3の訓練済みモジュールを読み込み\n",
    "    with tf.gfile.FastGFile(FLAGS.img_embedding_pb, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    img_embedding = tf.import_graph_def(graph_def, input_map={'input_image':img_input}, return_elements=['InceptionV3/Predictions/Reshape:0'])\n",
    "\n",
    "    #RNNの中間層に合わせるための全結合\n",
    "    img_embedding = tf.layers.dense(img_embedding[0], FLAGS.embedding_size)\n",
    "\n",
    "    return img_embedding\n",
    "\n",
    "\n",
    "def build_caption(img_embedding, vocab_size, is_train=True, decoder_input=None, decoder_lengths=None, end_token=EOS_ID):\n",
    "\n",
    "    with tf.name_scope(\"captioning\"):\n",
    "\n",
    "        batch_size = tf.shape(img_embedding)[0]\n",
    "        embedding_size = FLAGS.embedding_size\n",
    "        #単語組み込み用の重みを定義\n",
    "        word_embedding = tf.get_variable(\"embeddings\", [vocab_size , embedding_size])\n",
    "\n",
    "        if is_train:\n",
    "            #decoder_inputからlookup\n",
    "            embedded_input = tf.nn.embedding_lookup(word_embedding, decoder_input)\n",
    "            #decoder側への組み込み入力、有効時間長を引数に訓練用のヘルパーを定義\n",
    "            helper = tf.contrib.seq2seq.TrainingHelper(embedded_input, decoder_lengths)\n",
    "        else:\n",
    "            #出力層で最も確率が高いものだけを選択し次の入力にするヘルパーを定義\n",
    "            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(word_embedding, tf.fill([batch_size], SOS_ID), end_token)\n",
    "\n",
    "        #LSTMのCellを定義\n",
    "        rnn_cell = tf.nn.rnn_cell.BasicLSTMCell(embedding_size)\n",
    "        #画像組み込みベクトルを入力にしてRNNを1ステップ計算してキャプション生成の初期値獲得\n",
    "        _, initial_state = rnn_cell(img_embedding, rnn_cell.zero_state(batch_size, dtype=tf.float32))\n",
    "\n",
    "        #出力そうの挙動を定義\n",
    "        projection_layer = tf.layers.Dense(vocab_size, use_bias=False)\n",
    "        #デコーダー側の挙動を確定\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(rnn_cell, helper, initial_state, projection_layer)\n",
    "        #Dynamicデコード\n",
    "        output, final_state, lengths = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=100)\n",
    "\n",
    "    return output, lengths\n",
    "\n",
    "#誤差計測\n",
    "def build_loss(logit, correct, lengths):\n",
    "    with tf.name_scope('loss'):\n",
    "        #有効でない時系列部分をマスク\n",
    "        max_len = tf.reduce_max(lengths)\n",
    "        weight = tf.sequence_mask(lengths, max_len, logit.dtype)\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logit, correct, weight)\n",
    "    return loss\n",
    "\n",
    "#訓練実施\n",
    "def build_train(loss):\n",
    "    with tf.name_scope('train'):\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        #勾配クリッピング\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, FLAGS.max_gradient_norm)\n",
    "        #訓練\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        update = optimizer.apply_gradients(zip(clipped_gradients, params),global_step=global_step)\n",
    "\n",
    "    return update, global_step\n",
    "\n",
    "#ckptからのrestoreもしくは初期化\n",
    "def initialize_model(saver, sess, initializer=None):\n",
    "\n",
    "    #最新のckptを取得\n",
    "    ckpt_state = tf.train.get_checkpoint_state(FLAGS.model_dir)\n",
    "    if ckpt_state:\n",
    "        last_model = ckpt_state.model_checkpoint_path\n",
    "        saver.restore(sess,last_model)\n",
    "    else:\n",
    "        sess.run(initializer)\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "\n",
    "    dataset_dir = FLAGS.tfrecord_dir\n",
    "    embedding_size = FLAGS.embedding_size\n",
    "\n",
    "    #辞書ファイルの読み込み\n",
    "    print(\"Load dictionary.\")\n",
    "    id_to_word = []\n",
    "    with open(FLAGS.dictionary_path, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.rstrip('\\n')\n",
    "            id_to_word.append(word)\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    print(\"Start to build model.\")\n",
    "\n",
    "    train_graph = tf.Graph() #訓練用グラフ\n",
    "    infer_graph = tf.Graph() #推論用グラフ\n",
    "\n",
    "    #推論用のグラフ\n",
    "    with infer_graph.as_default():\n",
    "        #推論の入力はplaceholder形式\n",
    "        input_img = tf.placeholder(tf.float32,[None, 299, 299, 3], name='input_img')\n",
    "        infer_embedding = build_img_embedding(input_img, embedding_size)\n",
    "        infer_output, _ = build_caption(infer_embedding, vocab_size, is_train=False)\n",
    "\n",
    "        infer_saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    #訓練用のグラフ\n",
    "    with train_graph.as_default():\n",
    "        #訓練の入力はDataSetAPIを用いたパイプ\n",
    "        img , lengths, decoder_input, correct = build_input('train-*')\n",
    "        #画像組み込み\n",
    "        train_embedding = build_img_embedding(img, embedding_size)\n",
    "        #キャプション生成\n",
    "        train_output, train_lengths = build_caption(train_embedding, vocab_size, decoder_input=decoder_input, decoder_lengths=lengths)\n",
    "        #誤差計算\n",
    "        loss = build_loss(train_output.rnn_output, correct, lengths)\n",
    "        #訓練\n",
    "        update, global_step = build_train(loss)\n",
    "\n",
    "        #生成テキストのlogging\n",
    "        text_ph = tf.placeholder(tf.string, shape=(None,), name='generated')\n",
    "        tf.summary.text('text_summary', text_ph)\n",
    "        #ログのマージ\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        train_saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Finish building models.\")\n",
    "\n",
    "    #グラフごとにSessionを定義\n",
    "    sess_infer = tf.Session(graph=infer_graph)\n",
    "    sess_train = tf.Session(graph=train_graph)\n",
    "\n",
    "    #TensorBoardのwriterを定義\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, train_graph)\n",
    "\n",
    "    #inferenceをpb形式でエクスポート\n",
    "    tf.train.write_graph(infer_graph.as_graph_def(), './data', 'infer_graph.pb', as_text=False)\n",
    "\n",
    "    #初期化\n",
    "    initialize_model(train_saver, sess_train, initializer)\n",
    "\n",
    "    last_step = sess_train.run(global_step)\n",
    "    print(\"start training!\")\n",
    "    for i in range(FLAGS.max_step):\n",
    "        step = last_step + i + 1\n",
    "\n",
    "        sess_train.run(update)\n",
    "\n",
    "        #一定ステップごとにモデル保存とloggingを行い文章生成を行う\n",
    "        if step % FLAGS.eval_interval == 0:\n",
    "            print(\"Step%d. Save model.\"%step)\n",
    "            #モデル保存\n",
    "            train_saver.save(sess_train, FLAGS.model_dir+'mymodel')\n",
    "\n",
    "            #inference用のgraphにckptをロード\n",
    "            print(\"Restore to inference graph.\")\n",
    "            initialize_model(infer_saver, sess_infer)\n",
    "\n",
    "            #チェック用の画像ファイル一覧を取得\n",
    "            eval_images = []\n",
    "            for file_path in glob.glob(FLAGS.test_img_dir+'*.jpg'):\n",
    "                eval_images.append(file_path)\n",
    "\n",
    "            #画像をモデルの入力に変換\n",
    "            inference_input = []\n",
    "            for file in eval_images:\n",
    "                _img = Image.open(file)\n",
    "                inference_input.append(np.asarray(_img.resize((299,299)))/255.0)\n",
    "\n",
    "            #実行\n",
    "            infer_result = sess_infer.run(infer_output.sample_id, feed_dict={input_img: inference_input})\n",
    "            #結果を辞書で変換\n",
    "            captions = []\n",
    "            for result in infer_result:\n",
    "                caption = ''\n",
    "                for j in result:\n",
    "                    caption += id_to_word[j]\n",
    "                captions.append(caption)\n",
    "\n",
    "            #TensorBoardのlogging\n",
    "            run_opt = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_meta = tf.RunMetadata()\n",
    "            summary_val = sess_train.run(summary_op, feed_dict={text_ph:captions}, options=run_opt, run_metadata=run_meta)\n",
    "            summary_writer.add_summary(summary_val, step)\n",
    "            summary_writer.add_run_metadata(run_meta,'step%d'%step)\n",
    "\n",
    "\n",
    "    summary_writer.close()\n",
    "    sess_infer.close()\n",
    "    sess_train.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
